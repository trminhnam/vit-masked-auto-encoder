{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"bdebb647637b412686dd535593d851a0":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","model_module_version":"1.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_42d9e628edd9436a8852b3f0760abbbd","msg_id":"","outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[37mEpoch 0/4 \u001b[0m \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m628/628\u001b[0m \u001b[38;5;245m0:01:32 • 0:00:00\u001b[0m \u001b[38;5;249m6.78it/s\u001b[0m \u001b[37mv_num: 10 \u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Epoch 0/4 </span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">628/628</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:01:32 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">6.78it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">v_num: 10 </span>\n</pre>\n"},"metadata":{}}]}},"42d9e628edd9436a8852b3f0760abbbd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ATYhbi_3y-Og","executionInfo":{"status":"ok","timestamp":1682858939052,"user_tz":-420,"elapsed":30118,"user":{"displayName":"Minh Nam Tran","userId":"16434365196897396426"}},"outputId":"cf271a99-b895-4bde-92fc-392c743b1068"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/ViT-MaskedAttention'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n1rG0iI4zTmZ","executionInfo":{"status":"ok","timestamp":1682858939052,"user_tz":-420,"elapsed":11,"user":{"displayName":"Minh Nam Tran","userId":"16434365196897396426"}},"outputId":"389205ab-95b9-4ba0-94f4-f57eeade8916"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/19BEgVkLwJC08ZvXW8kbzitRp0_uGrU0x/ViT-MaskedAttention\n"]}]},{"cell_type":"markdown","source":["# Install pytorch-lightning"],"metadata":{"id":"UtnjwGAU7l5I"}},{"cell_type":"code","source":["!pip install -q pytorch-lightning\n","!pip install -q omegaconf\n","!pip install -q transformers"],"metadata":{"id":"Z8fEnKo77pfn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nwF3Zlh5BiZ2","executionInfo":{"status":"ok","timestamp":1682858965637,"user_tz":-420,"elapsed":27,"user":{"displayName":"Minh Nam Tran","userId":"16434365196897396426"}},"outputId":"f7ea2dad-b0a6-4aef-a538-d25d54609a7e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/19BEgVkLwJC08ZvXW8kbzitRp0_uGrU0x/ViT-MaskedAttention\n"]}]},{"cell_type":"markdown","source":["# Import Lib"],"metadata":{"id":"y5J3C3ib8qYq"}},{"cell_type":"code","source":["import os\n","import random\n","import sys\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.tensorboard import SummaryWriter\n","import torch.utils.data as data_utils\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision\n","import torchvision.models as models\n","from torchsummary import summary\n","\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","from pytorch_lightning.callbacks.lr_monitor import LearningRateMonitor\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score"],"metadata":{"id":"3gR8XFRR8sv8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Module\n"],"metadata":{"id":"6QLBX_jA8IMy"}},{"cell_type":"code","source":["class ImageDataset(Dataset):\n","    def __init__(self, df, transform=None):\n","        self.df = df\n","        self.transform = transform\n","        \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, idx):\n","        img = self.df.iloc[idx]['pixels']\n","        label = self.df.iloc[idx]['emotion']\n","        \n","        # Convert the numpy array to a PyTorch tensor\n","        img_tensor = torch.from_numpy(img).float().reshape(1, 48, 48)\n","        img_tensor = torch.cat([img_tensor, img_tensor, img_tensor], dim=0)\n","        \n","        if self.transform:\n","            img_tensor = self.transform(img_tensor)\n","            \n","        return img_tensor, label"],"metadata":{"id":"OOGwUDZLMZKP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import pytorch_lightning as pl\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","\n","mean = [0.4914, 0.4822, 0.4465]\n","std = [0.2470, 0.2435, 0.2616]\n","\n","class FERDataModule(pl.LightningDataModule):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.cfg = cfg\n","        self.data_dir = self.cfg.data.data_dir\n","        self.batch_size = self.cfg.data.batch_size\n","        self.num_workers = self.cfg.data.num_workers\n","        # Define data transformation pipeline with data augmentation\n","        self.transform_train = transforms.Compose([\n","            # transforms.ToPILImage(),\n","            # transforms.ToTensor(),\n","            # transforms.Resize(256),\n","            # transforms.RandomCrop(224),\n","            # transforms.RandomGrayscale(p=1),\n","            # transforms.RandomHorizontalFlip(),\n","            # transforms.RandomRotation(15),\n","            # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","            # transforms.RandomGrayscale(p=0.1),\n","            transforms.Normalize(mean, std)\n","        ])\n","\n","        self.transform_test = transforms.Compose([\n","            # transforms.ToTensor(),\n","            # transforms.Resize(256),\n","            # transforms.CenterCrop(224),\n","            transforms.Normalize(mean, std)\n","        ])\n","\n","    def prepare_data(self):\n","        pass\n","\n","    def setup(self, stage=None):\n","        # Assign train/val datasets for use in dataloaders\n","        if stage == 'fit' or stage is None:\n","            data = pd.read_csv(os.path.join(cfg.data.data_dir, 'train.csv'))\n","            data['pixels'] = data['pixels'].apply(lambda x : np.array(list(map(int, x.split(' ')))).reshape(48, 48))\n","            self.trainset, self.valset = train_test_split(data, train_size=0.7, random_state=42, stratify=data['emotion'], shuffle=True)\n","\n","            self.train_dataset = ImageDataset(self.trainset, transform=self.transform_train)\n","            self.val_dataset = ImageDataset(self.valset, transform=self.transform_train)\n","\n","        # Assign test dataset for use in dataloader(s)\n","        if stage == 'test' or stage is None:\n","            self.testset = pd.read_csv(os.path.join(cfg.data.data_dir, 'test.csv'))\n","            self.testset['pixels'] = self.testset['pixels'].apply(lambda x : np.array(list(map(int, x.split(' ')))).reshape(48, 48))\n","\n","            self.test_dataset = ImageDataset(self.testset, transform=self.transform_test)\n","\n","    def train_dataloader(self):\n","        return torch.utils.data.DataLoader(\n","            self.train_dataset,\n","            batch_size=self.batch_size,\n","            shuffle=True,\n","            num_workers=self.num_workers\n","        )\n","\n","    def val_dataloader(self):\n","        return torch.utils.data.DataLoader(\n","            self.val_dataset,\n","            batch_size=self.batch_size,\n","            shuffle=False,\n","            num_workers=self.num_workers\n","        )\n","\n","    def test_dataloader(self):\n","        return torch.utils.data.DataLoader(\n","            self.test_dataset,\n","            batch_size=self.batch_size,\n","            shuffle=False,\n","            num_workers=self.num_workers\n","        )"],"metadata":{"id":"B2qo2LhKE2b0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model\n"],"metadata":{"id":"-F1AcOTR8KnL"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","\n","\n","def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n","    \"\"\"\n","    grid_size: int of the grid height and width\n","    return:\n","    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n","    \"\"\"\n","    grid_h = np.arange(grid_size, dtype=np.float32)\n","    grid_w = np.arange(grid_size, dtype=np.float32)\n","    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n","    grid = np.stack(grid, axis=0)\n","\n","    grid = grid.reshape([2, 1, grid_size, grid_size])\n","    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n","    if cls_token:\n","        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n","    return pos_embed\n","\n","\n","def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n","    assert embed_dim % 2 == 0\n","\n","    # use half of dimensions to encode grid_h\n","    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n","    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n","\n","    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n","    return emb\n","\n","\n","def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n","    \"\"\"\n","    embed_dim: output dimension for each position\n","    pos: a list of positions to be encoded: size (M,)\n","    out: (M, D)\n","    \"\"\"\n","    assert embed_dim % 2 == 0\n","    omega = np.arange(embed_dim // 2, dtype=np.float)\n","    omega /= embed_dim / 2.0\n","    omega = 1.0 / 10000**omega  # (D/2,)\n","\n","    pos = pos.reshape(-1)  # (M,)\n","    out = np.einsum(\"m,d->md\", pos, omega)  # (M, D/2), outer product\n","\n","    emb_sin = np.sin(out)  # (M, D/2)\n","    emb_cos = np.cos(out)  # (M, D/2)\n","\n","    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n","    return emb\n","\n","\n","def interpolate_pos_embed(model, checkpoint_model):\n","    if \"pos_embed\" in checkpoint_model:\n","        pos_embed_checkpoint = checkpoint_model[\"pos_embed\"]\n","        embedding_size = pos_embed_checkpoint.shape[-1]\n","        num_patches = model.patch_embed.num_patches\n","        num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n","        # height (== width) for the checkpoint position embedding\n","        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n","        # height (== width) for the new position embedding\n","        new_size = int(num_patches**0.5)\n","        # class_token and dist_token are kept unchanged\n","        if orig_size != new_size:\n","            print(\n","                \"Position interpolate from %dx%d to %dx%d\"\n","                % (orig_size, orig_size, new_size, new_size)\n","            )\n","            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n","            # only the position tokens are interpolated\n","            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n","            pos_tokens = pos_tokens.reshape(\n","                -1, orig_size, orig_size, embedding_size\n","            ).permute(0, 3, 1, 2)\n","            pos_tokens = torch.nn.functional.interpolate(\n","                pos_tokens,\n","                size=(new_size, new_size),\n","                mode=\"bicubic\",\n","                align_corners=False,\n","            )\n","            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n","            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n","            checkpoint_model[\"pos_embed\"] = new_pos_embed"],"metadata":{"id":"o6g3q-U48XA7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class PatchEmbedding(nn.Module):\n","    def __init__(\n","        self,\n","        img_size,\n","        patch_size,\n","        in_channels=3,\n","        embedding_dim=768,\n","    ):\n","        super(PatchEmbedding, self).__init__()\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.n_patches = (img_size // patch_size) ** 2\n","\n","        self.projection_layer = nn.Conv2d(\n","            in_channels=in_channels,\n","            out_channels=embedding_dim,\n","            kernel_size=patch_size,\n","            stride=patch_size,\n","        )\n","\n","    def forward(self, x):\n","        \"\"\"Forward pass of patch embedding layer.\n","\n","        Args:\n","            x (torch.Tensor): input image tensor, shape (B, C, H, W)\n","        \"\"\"\n","\n","        x = self.projection_layer(\n","            x\n","        )  # (batch_size, embedding_dim, n_patches ** 0.5, n_patches ** 0.5)\n","        x = x.flatten(2)  # (batch_size, embedding_dim, n_patches)\n","        x = x.transpose(1, 2)  # (batch_size, n_patches, embedding_dim)\n","\n","        return x\n","\n","    def get_num_patches(self):\n","        return self.n_patches\n","\n","\n","class Attention(nn.Module):\n","    def __init__(\n","        self,\n","        d_model,\n","        n_heads,\n","        qkv_bias=False,\n","        attn_p=0.0,\n","        proj_p=0.0,\n","    ):\n","        super(Attention, self).__init__()\n","        self.d_model = d_model\n","        self.n_heads = n_heads\n","        self.head_dim = d_model // n_heads\n","        self.scale = self.head_dim**-0.5\n","\n","        self.qkv_linear = nn.Linear(d_model, d_model * 3, bias=qkv_bias)\n","        self.attn_dropout = nn.Dropout(attn_p)\n","\n","        self.proj_linear = nn.Linear(d_model, d_model)\n","        self.proj_dropout = nn.Dropout(proj_p)\n","\n","    def forward(self, x, q_mask=None):\n","        n_samples, n_patches, d_model = x.shape\n","\n","        if d_model != self.d_model:\n","            raise ValueError(\n","                f\"Input dimension {d_model} does not match layer dimension {self.d_model}\"\n","            )\n","\n","        qkv = self.qkv_linear(x)\n","        qkv = qkv.reshape(\n","            n_samples,\n","            n_patches,\n","            3,\n","            self.n_heads,\n","            self.head_dim,\n","        )  # (batch_size, n_patches, 3, n_heads, head_dim)\n","        qkv = qkv.permute(\n","            2, 0, 3, 1, 4\n","        )  # (3, batch_size, n_heads, n_patches, head_dim)\n","        q, k, v = qkv[0], qkv[1], qkv[2]  # (batch_size, n_heads, n_patches, head_dim)\n","        # print(f\"q.shape: {q.shape}\")\n","\n","        dp = (\n","            q @ k.transpose(-2, -1)\n","        ) * self.scale  # (batch_size, n_heads, n_patches, n_patches)\n","        # print(f\"dp.shape: {dp.shape}\")\n","\n","        if q_mask is not None:\n","            dp = dp.masked_fill(q_mask == 0, float(\"-inf\"))\n","\n","        attn = dp.softmax(dim=-1)\n","        attn = self.attn_dropout(attn)\n","\n","        weighted_avg = attn @ v  # (batch_size, n_heads, n_patches, head_dim)\n","        weighted_avg = weighted_avg.transpose(\n","            1, 2\n","        )  # (batch_size, n_patches, n_heads, head_dim)\n","        weighted_avg = weighted_avg.flatten(2)  # (batch_size, n_patches, d_model)\n","\n","        x = self.proj_linear(weighted_avg)\n","        x = self.proj_dropout(x)\n","\n","        return x\n","\n","\n","class MLP(nn.Module):\n","    def __init__(\n","        self,\n","        in_features,\n","        hidden_features,\n","        out_features,\n","        p=0.0,\n","    ):\n","        super(MLP, self).__init__()\n","        self.fc1 = nn.Linear(in_features, hidden_features)\n","        self.fc2 = nn.Linear(hidden_features, out_features)\n","        self.dropout = nn.Dropout(p)\n","        self.gelu = nn.GELU()\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.gelu(x)\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","        x = self.dropout(x)\n","        return x\n","\n","\n","class Block(nn.Module):\n","    def __init__(\n","        self,\n","        d_model,\n","        n_heads,\n","        mlp_ratio=4.0,\n","        qkv_bias=True,\n","        attn_p=0.0,\n","        proj_p=0.0,\n","    ):\n","        super(Block, self).__init__()\n","        self.d_model = d_model\n","        self.n_heads = n_heads\n","        self.mlp_ratio = mlp_ratio\n","        self.mlp_hidden_features = int(d_model * mlp_ratio)\n","\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.attn = Attention(\n","            d_model=d_model,\n","            n_heads=n_heads,\n","            qkv_bias=qkv_bias,\n","            attn_p=attn_p,\n","            proj_p=proj_p,\n","        )\n","\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.mlp = MLP(\n","            in_features=d_model,\n","            hidden_features=self.mlp_hidden_features,\n","            out_features=d_model,\n","        )\n","\n","    def forward(self, x, q_mask=None):\n","        x = x + self.attn(self.norm1(x), q_mask=q_mask)\n","        x = x + self.mlp(self.norm2(x))\n","\n","        return x\n","\n","\n","class ViT(nn.Module):\n","    def __init__(\n","        self,\n","        img_size=224,\n","        patch_size=16,\n","        in_channels=3,\n","        num_classes=1000,\n","        embedding_dim=768,\n","        depth=12,\n","        n_heads=12,\n","        mlp_ratio=4.0,\n","        qkv_bias=True,\n","        attn_p=0.0,\n","        p=0.0,\n","        fine_tune=False,\n","    ):\n","        super(ViT, self).__init__()\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.in_channels = in_channels\n","        self.num_classes = num_classes\n","        self.embedding_dim = embedding_dim\n","        self.depth = depth\n","\n","        # input\n","        self.patch_embed = PatchEmbedding(\n","            img_size=img_size,\n","            patch_size=patch_size,\n","            in_channels=in_channels,\n","            embedding_dim=embedding_dim,\n","        )\n","        self.cls_token = nn.Parameter(torch.zeros(1, 1, embedding_dim))\n","        self.pos_embed = nn.Parameter(\n","            torch.zeros(1, (img_size // patch_size) ** 2 + 1, embedding_dim)\n","        )\n","        self.pos_dropout = nn.Dropout(p=p)\n","\n","        # transformer\n","        self.blocks = nn.ModuleList(\n","            [\n","                Block(\n","                    d_model=embedding_dim,\n","                    n_heads=n_heads,\n","                    mlp_ratio=mlp_ratio,\n","                    qkv_bias=qkv_bias,\n","                    attn_p=attn_p,\n","                    proj_p=p,\n","                )\n","                for _ in range(depth)\n","            ]\n","        )\n","\n","        self.norm = nn.LayerNorm(embedding_dim)\n","        self.head = nn.Linear(embedding_dim, num_classes)\n","\n","    def forward(self, x, q_mask=None, get_features=False):\n","        batch_size = x.shape[0]\n","        x = self.patch_embed(x)\n","\n","        # add cls token and position embedding\n","        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n","        # print(f\"cls_tokens.shape: {cls_tokens.shape}\")\n","        # print(f\"x.shape: {x.shape}\")\n","        x = torch.cat((cls_tokens, x), dim=1)\n","        if q_mask is not None:\n","            q_mask = F.pad(q_mask, (1, 0), value=True)\n","        x = x + self.pos_embed\n","        x = self.pos_dropout(x)\n","\n","        # transformer\n","        for block in self.blocks:\n","            x = block(x, q_mask=q_mask)\n","\n","        transformer_features = self.norm(x)\n","\n","        # classification head\n","        x = self.head(transformer_features[:, 0])\n","\n","        if get_features:\n","            return x, transformer_features\n","        return x\n","\n","\n","class MaskedAutoencoderViT(nn.Module):\n","    \"\"\"Masked Autoencoder with VisionTransformer backbone\"\"\"\n","\n","    def __init__(\n","        self,\n","        img_size=224,\n","        patch_size=16,\n","        in_channels=3,\n","        num_classes=1000,\n","        embedding_dim=1024,\n","        depth=24,\n","        n_heads=16,\n","        decoder_embed_dim=512,\n","        decoder_depth=8,\n","        decoder_num_heads=16,\n","        mlp_ratio=4.0,\n","        qkv_bias=True,\n","        norm_layer=nn.LayerNorm,\n","        norm_pix_loss=False,\n","        attn_p=0.0,\n","        p=0.0,\n","    ):\n","        super().__init__()\n","\n","        # --------------------------------------------------------------------------\n","        # MAE encoder specifics\n","        self.patch_embed = PatchEmbedding(\n","            img_size, patch_size, in_channels, embedding_dim\n","        )\n","        num_patches = self.patch_embed.get_num_patches()\n","\n","        self.cls_token = nn.Parameter(torch.zeros(1, 1, embedding_dim))\n","        self.pos_embed = nn.Parameter(\n","            torch.zeros(1, num_patches + 1, embedding_dim), requires_grad=False\n","        )  # fixed sin-cos embedding\n","\n","        self.blocks = nn.ModuleList(\n","            [\n","                Block(\n","                    embedding_dim,\n","                    n_heads,\n","                    mlp_ratio,\n","                    qkv_bias=qkv_bias,\n","                    attn_p=attn_p,\n","                    proj_p=p,\n","                )\n","                for _ in range(depth)\n","            ]\n","        )\n","        self.norm = norm_layer(embedding_dim)\n","        self.cls_head = nn.Linear(embedding_dim, num_classes)  # classification head\n","        # --------------------------------------------------------------------------\n","\n","        # --------------------------------------------------------------------------\n","        # MAE decoder specifics\n","        self.decoder_embed = nn.Linear(embedding_dim, decoder_embed_dim, bias=True)\n","\n","        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n","\n","        self.decoder_pos_embed = nn.Parameter(\n","            torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False\n","        )  # fixed sin-cos embedding\n","\n","        self.decoder_blocks = nn.ModuleList(\n","            [\n","                Block(\n","                    decoder_embed_dim,\n","                    decoder_num_heads,\n","                    mlp_ratio,\n","                    qkv_bias=qkv_bias,\n","                    attn_p=attn_p,\n","                    proj_p=p,\n","                )\n","                for _ in range(decoder_depth)\n","            ]\n","        )\n","\n","        self.decoder_norm = norm_layer(decoder_embed_dim)\n","        self.decoder_pred = nn.Linear(\n","            decoder_embed_dim, patch_size**2 * in_channels, bias=True\n","        )  # decoder to patch\n","        # --------------------------------------------------------------------------\n","\n","        self.norm_pix_loss = norm_pix_loss\n","\n","        self.initialize_weights()\n","\n","    def forward(self, imgs, stage, mask_ratio=0.75):\n","        if stage == \"pretrain\":\n","            latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio, stage)\n","            pred = self.forward_decoder(latent, ids_restore)  # [N, L, p*p*3]\n","            loss = self.forward_loss(imgs, pred, mask)\n","            # return loss, pred, mask\n","            return {\"loss\": loss, \"pred\": pred, \"mask\": mask}\n","\n","        elif stage == \"finetune\":\n","            latent, _, _ = self.forward_encoder(imgs, mask_ratio, stage)\n","            cls_token = latent[:, 0, :]\n","            pred_cls = self.cls_head(cls_token)\n","            # return pred_cls\n","            return {\"pred_cls\": pred_cls}\n","\n","        elif stage == \"combine\":\n","            latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio, stage)\n","            pred = self.forward_decoder(latent, ids_restore)  # [N, L, p*p*3]\n","            loss = self.forward_loss(imgs, pred, mask)\n","            cls_token = latent[:, 0, :]\n","            pred_cls = self.cls_head(cls_token)\n","            # return loss, pred, mask, pred_cls\n","            return {\n","                \"loss\": loss,\n","                \"pred\": pred,\n","                \"mask\": mask,\n","                \"pred_cls\": pred_cls,\n","            }\n","\n","        else:\n","            raise ValueError(\"stage must be one of 'pretrain', 'finetune', 'combine'\")\n","\n","    def classify(self, imgs):\n","        latent, _, _ = self.forward_encoder(imgs, mask_ratio=0.75, stage=\"finetune\")\n","        cls_token = latent[:, 0, :]\n","        pred = self.cls_head(cls_token)\n","        pred = F.softmax(pred, dim=-1)\n","        pred = pred.argmax(dim=-1)\n","        return pred\n","\n","    def predict(self, imgs):\n","        return self.classify(imgs)\n","\n","    def forward_encoder(self, x, mask_ratio, stage):\n","        # embed patches\n","        x = self.patch_embed(x)\n","\n","        # add pos embed w/o cls token\n","        x = x + self.pos_embed[:, 1:, :]\n","\n","        # masking: length -> length * mask_ratio\n","        if stage != \"finetune\":\n","            x, mask, ids_restore = self.random_masking(x, mask_ratio)\n","        else:\n","            mask = None\n","            ids_restore = None\n","\n","        # append cls token\n","        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n","        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n","        x = torch.cat((cls_tokens, x), dim=1)\n","\n","        # apply Transformer blocks\n","        for blk in self.blocks:\n","            x = blk(x)\n","        x = self.norm(x)\n","\n","        return x, mask, ids_restore\n","\n","    def forward_decoder(self, x, ids_restore):\n","        # embed tokens\n","        x = self.decoder_embed(x)\n","\n","        # append mask tokens to sequence\n","        mask_tokens = self.mask_token.repeat(\n","            x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1\n","        )\n","        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n","        x_ = torch.gather(\n","            x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2])\n","        )  # unshuffle\n","        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n","\n","        # add pos embed\n","        x = x + self.decoder_pos_embed\n","\n","        # apply Transformer blocks\n","        for blk in self.decoder_blocks:\n","            x = blk(x)\n","        x = self.decoder_norm(x)\n","\n","        # predictor projection\n","        x = self.decoder_pred(x)\n","\n","        # remove cls token\n","        x = x[:, 1:, :]\n","\n","        return x\n","\n","    def forward_loss(self, imgs, pred, mask):\n","        \"\"\"\n","        imgs: [N, 3, H, W]\n","        pred: [N, L, p*p*3]\n","        mask: [N, L], 0 is keep, 1 is remove,\n","        \"\"\"\n","        target = self.patchify(imgs)\n","        if self.norm_pix_loss:\n","            mean = target.mean(dim=-1, keepdim=True)\n","            var = target.var(dim=-1, keepdim=True)\n","            target = (target - mean) / (var + 1.0e-6) ** 0.5\n","\n","        loss = (pred - target) ** 2\n","        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n","\n","        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n","        return loss\n","\n","    def initialize_weights(self):\n","        num_patches = self.patch_embed.get_num_patches()\n","        pos_embed = get_2d_sincos_pos_embed(\n","            self.pos_embed.shape[-1],\n","            int(num_patches**0.5),\n","            cls_token=True,\n","        )\n","        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n","\n","        decoder_pos_embed = get_2d_sincos_pos_embed(\n","            self.decoder_pos_embed.shape[-1],\n","            int(num_patches**0.5),\n","            cls_token=True,\n","        )\n","        self.decoder_pos_embed.data.copy_(\n","            torch.from_numpy(decoder_pos_embed).float().unsqueeze(0)\n","        )\n","\n","        w = self.patch_embed.projection_layer.weight.data\n","        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n","\n","        torch.nn.init.normal_(self.cls_token, std=0.02)\n","        torch.nn.init.normal_(self.mask_token, std=0.02)\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            # we use xavier_uniform following official JAX ViT:\n","            torch.nn.init.xavier_uniform_(m.weight)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    def random_masking(self, x, mask_ratio):\n","        \"\"\"\n","        Perform per-sample random masking by per-sample shuffling.\n","        Per-sample shuffling is done by argsort random noise.\n","        x: [N, L, D], sequence\n","        \"\"\"\n","        N, L, D = x.shape  # batch, length, dim\n","        len_keep = int(L * (1 - mask_ratio))\n","\n","        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n","\n","        # sort noise for each sample\n","        ids_shuffle = torch.argsort(\n","            noise, dim=1\n","        )  # ascend: small is keep, large is remove\n","        ids_restore = torch.argsort(ids_shuffle, dim=1)\n","\n","        # keep the first subset\n","        ids_keep = ids_shuffle[:, :len_keep]\n","        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n","\n","        # generate the binary mask: 0 is keep, 1 is remove\n","        mask = torch.ones([N, L], device=x.device)\n","        mask[:, :len_keep] = 0\n","        # unshuffle to get the binary mask\n","        mask = torch.gather(mask, dim=1, index=ids_restore)\n","\n","        return x_masked, mask, ids_restore\n","\n","    def patchify(self, imgs):\n","        \"\"\"\n","        imgs: (N, 3, H, W)\n","        x: (N, L, patch_size**2 *3)\n","        \"\"\"\n","        p = self.patch_embed.patch_size\n","        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n","\n","        h = w = imgs.shape[2] // p\n","        x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))\n","        x = torch.einsum(\"nchpwq->nhwpqc\", x)\n","        x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 3))\n","        return x\n","\n","    def unpatchify(self, x):\n","        \"\"\"\n","        x: (N, L, patch_size**2 *3)\n","        imgs: (N, 3, H, W)\n","        \"\"\"\n","        p = self.patch_embed.patch_size\n","        h = w = int(x.shape[1] ** 0.5)\n","        assert h * w == x.shape[1]\n","\n","        x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n","        x = torch.einsum(\"nhwpqc->nchpwq\", x)\n","        imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n","        return imgs"],"metadata":{"id":"BRl5p0sb8Mh0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Lightning Module"],"metadata":{"id":"XQAjmcry8cOj"}},{"cell_type":"code","source":["class VITMaskedAttetionModule(pl.LightningModule):\n","    \n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.cfg = cfg\n","        config = {\n","            \"img_size\": cfg.model.img_size,\n","            \"patch_size\": cfg.model.patch_size,\n","            \"in_channels\": cfg.model.in_channels,\n","            \"num_classes\": cfg.model.num_classes,\n","            \"embedding_dim\": cfg.model.embedding_dim,\n","            \"depth\": cfg.model.depth,\n","            \"n_heads\": cfg.model.n_heads,\n","            \"mlp_ratio\": cfg.model.mlp_ratio,\n","            \"qkv_bias\": cfg.model.qkv_bias,\n","            \"attn_p\": cfg.model.attn_p,\n","            \"p\": cfg.model.p,\n","        }\n","        self.model = MaskedAutoencoderViT(**config)\n","        self.save_dir = self.cfg.train.save_dir\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y = batch\n","        if self.cfg.stage == 'pretrain':\n","            y_hat = self.model(x, mask_ratio=0.5, stage=\"pretrain\")\n","            loss = y_hat['loss']\n","        elif self.cfg.stage == 'finetune':\n","            y_hat = self.model(x, mask_ratio=0.5, stage=\"finetune\")\n","            loss = F.cross_entropy(y_hat[\"pred_cls\"], y)\n","        elif self.cfg.stage == 'combine':\n","            y_hat = self.model(x, mask_ratio=0.5, stage=\"combine\")\n","            loss = F.cross_entropy(y_hat[\"pred_cls\"], y) + y_hat['loss']\n","        else:\n","            raise \"No such stage\"\n","        self.log('train_loss', loss)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y = batch\n","        if self.cfg.stage == 'pretrain':\n","            y_hat = self.model(x, mask_ratio=0.5, stage=\"pretrain\")\n","            loss = y_hat['loss']\n","        elif self.cfg.stage == 'finetune':\n","            y_hat = self.model(x, mask_ratio=0.5, stage=\"finetune\")\n","            loss = F.cross_entropy(y_hat[\"pred_cls\"], y)\n","        elif self.cfg.stage == 'combine':\n","            y_hat = self.model(x, mask_ratio=0.5, stage=\"combine\")\n","            loss = F.cross_entropy(y_hat[\"pred_cls\"], y) + y_hat['loss']\n","        else:\n","            raise \"No such stage\"\n","        self.log('val_loss', loss)\n","\n","    def test_step(self, batch, batch_idx):\n","        x, y = batch\n","        # y_hat = self.model(x)\n","        # loss = F.cross_entropy(y_hat, y)\n","        # preds = torch.argmax(y_hat, dim=1)\n","        # acc = (y_hat.argmax(dim=1) == y).float().mean()\n","        # self.log('test_loss', loss, prog_bar=True, logger=True)\n","        # self.log('test_acc', acc, prog_bar=True, logger=True)\n","        # # self.log('test_auc', roc_auc_score(y.cpu(), F.softmax(y_hat.cpu(), dim=1), multi_class='ovo'), prog_bar=True, logger=True)\n","        # # self.log('test_confusion_matrix', torch.from_numpy(confusion_matrix(y.cpu(), preds.cpu())), prog_bar=True, logger=True)\n","        # self.log('test_precision', precision_score(y.cpu(), preds.cpu(), average='weighted'), prog_bar=True, logger=True)\n","        # self.log('test_recall', recall_score(y.cpu(), preds.cpu(), average='weighted'), prog_bar=True, logger=True)\n","        # self.log('test_f1', f1_score(y.cpu(), preds.cpu(), average='weighted'), prog_bar=True, logger=True)\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=self.cfg.train.lr, \n","                                                        weight_decay=self.cfg.train.weight_decay)\n","        return optimizer\n","    \n","    def configure_callbacks(self):\n","        early_stop_callback = EarlyStopping(monitor='val_loss', min_delta=0.00, patience=3, verbose=False, mode='min')\n","        checkpoint_callback = ModelCheckpoint(monitor='val_loss', dirpath=self.save_dir, filename=f'{self.cfg.data.name}-{self.cfg.stage}'+'{epoch:02d}-{val_loss:.2f}', save_top_k=3, mode='min')\n","        return [early_stop_callback, checkpoint_callback]"],"metadata":{"id":"PK-JGkuR8gzM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Configuration"],"metadata":{"id":"nTqze03KRnv2"}},{"cell_type":"code","source":["import yaml\n","\n","with open('configs/ViTMaskedAttention-FER.YAML') as f:\n","    base_config = yaml.safe_load(f)\n","base_config"],"metadata":{"id":"ZDDha3afRnjT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["base_pretrained_config_path = 'configs/base-pretrain.yaml'\n","\n","# base pretrained config\n","base_pretrained_config = {k:v for k, v in base_config.items()}\n","base_pretrained_config['model']['img_size'] = 96\n","base_pretrained_config['model']['patch_size'] = 32\n","with open(base_pretrained_config_path, mode='w') as f:\n","    yaml.dump(base_pretrained_config, f)"],"metadata":{"id":"aeq2DEEDRuAH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["large_pretrained_config_path = 'configs/large-pretrain.yaml'\n","\n","large_pretrained_config = {k:v for k, v in base_config.items()}\n","large_pretrained_config['model']['mlp_ratio'] = 4.0\n","large_pretrained_config['model']['embedding_dim'] = 1024\n","large_pretrained_config['model']['depth'] = 12\n","large_pretrained_config['model']['n_heads'] = 8\n","\n","with open(large_pretrained_config_path, mode='w') as f:\n","    yaml.dump(large_pretrained_config, f)"],"metadata":{"id":"bZ5obK-NRv6C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training\n"],"metadata":{"id":"QybiImnR-JMD"}},{"cell_type":"code","source":["from pytorch_lightning.loggers import TensorBoardLogger\n","from pytorch_lightning.callbacks import RichProgressBar\n","from pytorch_lightning.callbacks.progress.rich_progress import RichProgressBarTheme\n","\n","from omegaconf import OmegaConf, DictConfig\n","\n","cfg_path = './configs/base-pretrain.YAML'\n","cfg = OmegaConf.load(cfg_path)\n","\n","logger = TensorBoardLogger(\"tb_logs\", name=\"fer\")\n","progress_bar = RichProgressBar()\n","\n","model = VITMaskedAttetionModule(cfg)\n","# Load from checkpoint\n","# model = VITMaskedAttetionModule.load_from_checkpoint(\"/path/to/checkpoint.ckpt\", cfg=cfg)\n","datamodule = FERDataModule(cfg)\n","\n","trainer = pl.Trainer(\n","    accelerator=\"auto\",\n","    max_epochs=cfg.train.epochs,\n","    deterministic=True,\n","    num_sanity_val_steps=0,\n","    logger=logger,\n","    callbacks=[progress_bar]\n",")\n","trainer.fit(model=model, datamodule=datamodule)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["bdebb647637b412686dd535593d851a0","42d9e628edd9436a8852b3f0760abbbd"]},"id":"xoZ2gkFe-Otf","outputId":"d24779fa-3186-47b7-c0e9-85ec580a7f8a","executionInfo":{"status":"error","timestamp":1682859239914,"user_tz":-420,"elapsed":133519,"user":{"displayName":"Minh Nam Tran","userId":"16434365196897396426"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-8-def9c96fe886>:41: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  omega = np.arange(embed_dim // 2, dtype=np.float)\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n","INFO:pytorch_lightning.utilities.rank_zero:The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n","INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"output_type":"display_data","data":{"text/plain":["┏━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n","┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n","┡━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n","│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model │ MaskedAutoencoderViT │ 46.6 M │\n","└───┴───────┴──────────────────────┴────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n","┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                 </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n","┡━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n","│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model │ MaskedAutoencoderViT │ 46.6 M │\n","└───┴───────┴──────────────────────┴────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1mTrainable params\u001b[0m: 46.6 M                                                                                           \n","\u001b[1mNon-trainable params\u001b[0m: 12.8 K                                                                                       \n","\u001b[1mTotal params\u001b[0m: 46.6 M                                                                                               \n","\u001b[1mTotal estimated model params size (MB)\u001b[0m: 186                                                                        \n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 46.6 M                                                                                           \n","<span style=\"font-weight: bold\">Non-trainable params</span>: 12.8 K                                                                                       \n","<span style=\"font-weight: bold\">Total params</span>: 46.6 M                                                                                               \n","<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 186                                                                        \n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"display_data","data":{"text/plain":["Output()"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdebb647637b412686dd535593d851a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":[],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n","</pre>\n"]},"metadata":{}},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-cd3e80567900>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m )\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatamodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_unwrap_optimized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lightning_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    521\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         )\n\u001b[0;32m--> 559\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    933\u001b[0m         \u001b[0;31m# RUN THE TRAINER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    976\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unexpected state {self.state}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36mon_advance_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_callback_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"on_train_epoch_end\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitoring_callbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_lightning_module_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"on_train_epoch_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_callback_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"on_train_epoch_end\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitoring_callbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_callback_hooks\u001b[0;34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Callback]{callback.state_key}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m                 \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpl_module\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36mon_train_epoch_end\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0mmonitor_candidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_monitor_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_every_n_epochs\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_epoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_every_n_epochs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_topk_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_last_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36m_save_topk_checkpoint\u001b[0;34m(self, trainer, monitor_candidates)\u001b[0m\n\u001b[1;32m    358\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mMisconfigurationException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m                 \u001b[0mwarning_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_monitor_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_none_monitor_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36m_save_monitor_checkpoint\u001b[0;34m(self, trainer, monitor_candidates)\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_monitor_top_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_best_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmonitor_candidates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36m_update_best_and_save\u001b[0;34m(self, current, trainer, monitor_candidates)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mcurrent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inf\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"min\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"-inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m         \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_metric_interpolated_filepath_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor_candidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdel_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;31m# save the current score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36m_get_metric_interpolated_filepath_name\u001b[0;34m(self, monitor_candidates, trainer, del_filepath)\u001b[0m\n\u001b[1;32m    615\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor_candidates\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdel_filepath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m     ) -> str:\n\u001b[0;32m--> 617\u001b[0;31m         \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_checkpoint_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m         \u001b[0mversion_cnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTARTING_VERSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36mformat_checkpoint_name\u001b[0;34m(self, metrics, filename, ver)\u001b[0m\n\u001b[1;32m    560\u001b[0m         \"\"\"\n\u001b[1;32m    561\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_checkpoint_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_insert_metric_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_insert_metric_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mver\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36m_format_checkpoint_name\u001b[0;34m(cls, filename, metrics, prefix, auto_insert_metric_name)\u001b[0m\n\u001b[1;32m    521\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m                     \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_meta\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Invalid format specifier"]}]}]}